
![](./images/data_cleaning.png)
# <center> Анализ резюме из HeadHunter </center>

## Оглавление
1. [Описание проекта](#Описание-проекта)
2. [Описание данных](#Описание-данных)
3. [Зависимости](#Зависимости)
4. [Установка проекта](#Установка-проекта)
5. [Использование проекта](#Использование-проекта)
6. [Авторы](#Авторы)
7. [Выводы](Использование-проекта)

## Описание проекта
Компания HeadHunter хочет построить модель, которая бы автоматически определяла примерный уровень заработной платы, подходящей пользователю, исходя из информации, которую он указал о себе. Однако прежде чем построить модель, данные необходимо преобразовать, исследовать и очистить. В данном проекте и прведены требуемые преобразования

Проект состоит из четырёх частей: 
    1 базовый анализ структуры данных 
    2 преобразование данных 
    3 разведывательный анализ 
    4 очистка данных 
    
Каждая часть состоит из блока практических заданий, которые выполнены в jupyter-ноутбуке.

> **Очистка данных (data cleaning)** – это процесс обнаружения и удаления (или исправления) поврежденных, ложных или неинформативных записей таблицы или целой базы данных. Процесс состоит из двух этапов: поиск и ликвидация (или редактирование).

Основные этапы очистки данных:
* Работа с пропущенными значениями.
* Очистка данных от пропусков.
* Удаление признаков и записей, которые не несут полезной информации.

**Цель очистки данных** — избавиться от «мусора», который может помешать моделированию или исказить его результаты. Во многих задачах очистка данных — это самая главная часть этапа подготовки данных к построению модели, которая нередко занимает большую часть времени работы над задачей.

**Данный проект** направлен на демонстрацию применения различных методов преобразования, исследования, очистки данных на каждом из ее этапов на примере датасета оискателей Headhunter.

**О структуре проекта:**
* [data](./data) - папка с исходными табличными данными
* [images](./images) - папка с изображениями, необходимыми для проекта
* [Headhunter_Project_Ноутбук.ipynb](./Headhunter_Project_Ноутбук.ipynb) - jupyter-ноутбук, содержащий основной код проекта, в котором демонстрируются методы и подходы решения задач исследования и очистки данных


## Описание данных
В проекте используется база резюме, выгруженная с сайта поиска вакансий hh.ru.
Датасет содержит 44744 обезличенные записи по 12 признакам:
    Пол, возраст - пол и возраст соискателя;
    ЗП - ожидаемая заработная плата;
    Ищет работу на должность: - сведенья о желаемой должности;
    Город, переезд, командировки - город проживания соискателя, его готовность к переезду и командировкам;
    Занятость - желаемая занятость в виде одной из категорий: полная занятость, частичная занятость, проектная работа, волонтерство, стажировка;
    График - желаемый график работы в виде одной из категорий: полный день, сменный график, гибкий график, удаленная работа, вахтовый метод;
    Опыт работы - сведенья об опыте работы соискателя;
    Последнее/нынешнее место работы - сведенья последнем/нынешнем месте работы;
    Последняя/нынешняя должность - сведенья о последней/нынешней должности;
    Образование и ВУЗ - уровень образования соискателя и наименование законченного учебного заведения;
    Обновление резюме - дата и время последнего обновления резюме соискателем;
    Авто - наличие у соискателя автомобиля.

Файл ExchangeRates.csv содержит сведенья о курсах валют.


## Используемые зависимости
* Python (3.9):
    * [numpy (1.20.3)](https://numpy.org)
    * [pandas (1.3.4)](https://pandas.pydata.org)
    * [matplotlib (3.4.3)](https://matplotlib.org)
    * [plotly.express (6.0.0)](https://plotly.com/python/plotly-express/)

## Установка проекта


## Использование
Вся информация о работе представлена в jupyter-ноутбуке Headhunter_Project_Ноутбук.ipynb.


## Выводы

В данном проекте была проведена работа по исследованию и очистке данных на примере датасета содержащего резюме соискателей с сайта поиска вакансий hh.ru. Было проведено преобразование данных путем формирования новых новых информативных признаков и удаления исходных, не несущих полезной информации. Выполнено исследование зависимостей в данных с использованием визуализации (бибилиотеки matplotlib, seaborn и plotly). Проведена очистка данных: удалены дублированные записи, проведена обработка пропусков в данных, ликвидированы выбросы.